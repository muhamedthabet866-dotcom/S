import streamlit as st
import pandas as pd
import numpy as np
import re
import base64
from datetime import datetime
from collections import OrderedDict
import hashlib

# Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙØ­Ø© Streamlit
st.set_page_config(
    page_title="Advanced SPSS Code Generator",
    page_icon="ğŸ“Š",
    layout="wide"
)

st.title("ğŸ“Š Ù…ÙˆÙ„Ø¯ Ø£ÙƒÙˆØ§Ø¯ SPSS Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
st.markdown("### Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø°ÙƒÙŠ ÙŠØ¹Ø§Ù„Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆÙŠÙˆÙ„Ù‘Ø¯ ÙƒÙˆØ¯ ÙØ±ÙŠØ¯ Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„")

class AdvancedSPSSGenerator:
    def __init__(self):
        self.processed_questions = OrderedDict()
        self.variable_analysis_cache = {}
        self.question_types = {
            'descriptive': ['mean', 'average', 'median', 'mode', 'standard deviation', 'variance', 'descriptive'],
            'frequency': ['frequency', 'distribution', 'count', 'table', 'percentage', 'percent'],
            't_test': ['t-test', 't test', 'compare means', 'independent samples', 'paired'],
            'anova': ['anova', 'analysis of variance', 'f-test', 'one-way', 'two-way'],
            'correlation': ['correlation', 'relationship', 'association', 'correlate'],
            'regression': ['regression', 'predict', 'linear model', 'multiple regression'],
            'chi_square': ['chi-square', 'chi squared', 'contingency', 'association categorical'],
            'graph': ['graph', 'chart', 'histogram', 'bar chart', 'pie chart', 'scatter', 'plot'],
            'confidence': ['confidence interval', 'ci', '95%', '99%', 'interval'],
            'normality': ['normality', 'normal distribution', 'shapiro-wilk', 'kolmogorov'],
            'outliers': ['outliers', 'extreme values', 'unusual observations'],
            'group_comparison': ['by group', 'for each', 'compare groups', 'between groups'],
            'recode': ['recode', 'categorize', 'group into', 'create classes', 'classify'],
            'transform': ['transform', 'compute', 'create variable', 'new variable', 'calculate']
        }
    
    def analyze_dataset(self, df):
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        analysis = {
            'variables': {},
            'summary': {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'numeric_vars': [],
                'categorical_vars': [],
                'text_vars': []
            }
        }
        
        for column in df.columns:
            col_data = df[column]
            var_info = {
                'name': column,
                'type': 'unknown',
                'missing': int(col_data.isna().sum()),
                'missing_percent': round(col_data.isna().sum() / len(df) * 100, 2),
                'unique_values': int(col_data.nunique())
            }
            
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù‚Ù…
                numeric_data = pd.to_numeric(col_data.dropna())
                var_info['type'] = 'numeric'
                var_info['min'] = float(numeric_data.min())
                var_info['max'] = float(numeric_data.max())
                var_info['mean'] = float(numeric_data.mean())
                var_info['std'] = float(numeric_data.std())
                
                if var_info['unique_values'] <= 10:
                    var_info['subtype'] = 'categorical_numeric'
                    var_info['values'] = sorted(numeric_data.unique())
                    analysis['summary']['categorical_vars'].append(column)
                else:
                    var_info['subtype'] = 'continuous'
                    analysis['summary']['numeric_vars'].append(column)
                    
            except:
                # Ù…ØªØºÙŠØ± Ù†ØµÙŠ
                var_info['type'] = 'text'
                if var_info['unique_values'] <= 15:
                    var_info['subtype'] = 'categorical_text'
                    var_info['values'] = list(col_data.dropna().unique())[:10]
                    analysis['summary']['categorical_vars'].append(column)
                else:
                    var_info['subtype'] = 'free_text'
                    analysis['summary']['text_vars'].append(column)
            
            analysis['variables'][column] = var_info
        
        return analysis
    
    def extract_variables_from_text(self, text, variable_names):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ø¯Ù‚Ø©"""
        text_lower = text.lower()
        detected_vars = []
        
        for var in variable_names:
            var_lower = var.lower()
            
            # Ø·Ø±Ù‚ Ù…Ø®ØªÙ„ÙØ© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
            patterns = [
                f'\\b{var_lower}\\b',  # Ø§Ù„ÙƒÙ„Ù…Ø© ÙƒØ§Ù…Ù„Ø©
                f'{var_lower}\\s+',    # Ù…ØªØ¨ÙˆØ¹ Ø¨Ù…Ø³Ø§ÙØ©
                f'\\s+{var_lower}\\b',  # Ù…Ø³Ø¨ÙˆÙ‚ Ø¨Ù…Ø³Ø§ÙØ©
                var_lower.replace('_', ' '),  # Ù…Ø¹ underscores
            ]
            
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    detected_vars.append(var)
                    break
            
            # Ø£ÙŠØ¶Ø§ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ø§Ø³Ù…
            if '_' in var:
                parts = var_lower.split('_')
                if any(part in text_lower for part in parts if len(part) > 2):
                    detected_vars.append(var)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
        return list(OrderedDict.fromkeys(detected_vars))
    
    def classify_question(self, question):
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¯Ù‚Ø© Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙØ±Ø¹ÙŠØ©"""
        question_lower = question.lower()
        classifications = []
        
        for q_type, keywords in self.question_types.items():
            for keyword in keywords:
                if re.search(r'\b' + re.escape(keyword) + r'\b', question_lower):
                    classifications.append(q_type)
                    break
        
        # ÙƒØ´Ù Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø®Ø§ØµØ©
        if 'graph' in classifications:
            if 'histogram' in question_lower:
                classifications.append('histogram')
            if 'bar' in question_lower and 'chart' in question_lower:
                classifications.append('bar_chart')
            if 'pie' in question_lower:
                classifications.append('pie_chart')
            if 'scatter' in question_lower:
                classifications.append('scatter_plot')
        
        return list(OrderedDict.fromkeys(classifications)) if classifications else ['general_analysis']
    
    def generate_question_fingerprint(self, question, detected_vars, classifications):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ØµÙ…Ø© ÙØ±ÙŠØ¯Ø© Ù„Ù„Ø³Ø¤Ø§Ù„ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
        components = [
            ' '.join(sorted(classifications)),
            ' '.join(sorted(detected_vars)),
            re.sub(r'\s+', ' ', question.lower()).strip()
        ]
        
        fingerprint_string = '|'.join(components)
        return hashlib.md5(fingerprint_string.encode()).hexdigest()[:8]
    
    def generate_spss_for_question(self, q_num, question, df, data_analysis):
        """ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ SPSS Ø®Ø§Øµ ÙˆÙ…Ø®ØªÙ„Ù Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„"""
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
        variable_names = list(df.columns)
        detected_vars = self.extract_variables_from_text(question, variable_names)
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„
        classifications = self.classify_question(question)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ØµÙ…Ø© Ø§Ù„Ø³Ø¤Ø§Ù„
        fingerprint = self.generate_question_fingerprint(question, detected_vars, classifications)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø± Ø§Ù„Ø³Ø¤Ø§Ù„
        if fingerprint in self.processed_questions:
            similar_q = self.processed_questions[fingerprint]
            return None, f"Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù„Ø³Ø¤Ø§Ù„ {similar_q['number']}. ØªÙ… ØªØ¬Ù†Ø¨Ù‡ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±."
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ØµÙ…Ø©
        self.processed_questions[fingerprint] = {
            'number': q_num,
            'question': question[:100],
            'variables': detected_vars,
            'types': classifications
        }
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯
        code_lines = []
        code_lines.append(f"* {'='*70}")
        code_lines.append(f"* QUESTION {q_num}: {question[:80]}{'...' if len(question) > 80 else ''}")
        code_lines.append(f"* Types: {', '.join(classifications)}")
        if detected_vars:
            code_lines.append(f"* Variables detected: {', '.join(detected_vars)}")
        code_lines.append(f"* {'='*70}\n")
        
        # Ø¥Ø¶Ø§ÙØ© ØªØ¹Ù„ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ÙŠ
        code_lines.append(f"* ANALYSIS FOR QUESTION {q_num}")
        
        # ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
        if not detected_vars:
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…ØªØºÙŠØ±Ø§Øª
            code_lines.append("* No specific variables detected in question.")
            code_lines.append("* Running general descriptive analysis on all variables.")
            code_lines.append("DESCRIPTIVES VARIABLES=ALL")
            code_lines.append("  /STATISTICS=MEAN STDDEV MIN MAX.\n")
        
        else:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª
            processed_commands = []
            
            for q_type in classifications:
                if q_type == 'descriptive':
                    if detected_vars:
                        vars_str = ' '.join(detected_vars[:5])
                        cmd = f"DESCRIPTIVES VARIABLES={vars_str}"
                        cmd += "\n  /STATISTICS=MEAN STDDEV MIN MAX SEMEAN KURTOSIS SKEWNESS."
                        if cmd not in processed_commands:
                            code_lines.append(cmd)
                            processed_commands.append(cmd)
                    
                elif q_type == 'frequency':
                    categorical_vars = [v for v in detected_vars 
                                      if data_analysis['variables'].get(v, {}).get('subtype') in 
                                      ['categorical_numeric', 'categorical_text']]
                    if categorical_vars:
                        vars_str = ' '.join(categorical_vars[:5])
                        cmd = f"FREQUENCIES VARIABLES={vars_str}"
                        cmd += "\n  /ORDER=ANALYSIS"
                        cmd += "\n  /BARCHART"
                        cmd += "\n  /PIECHART."
                        if cmd not in processed_commands:
                            code_lines.append(cmd)
                            processed_commands.append(cmd)
                    
                elif q_type == 'histogram':
                    numeric_vars = [v for v in detected_vars 
                                  if data_analysis['variables'].get(v, {}).get('subtype') == 'continuous']
                    for var in numeric_vars[:3]:
                        cmd = f"GRAPH /HISTOGRAM(NORMAL)={var}"
                        cmd += f"\n  /TITLE='Histogram of {var}'."
                        if cmd not in processed_commands:
                            code_lines.append(cmd)
                            processed_commands.append(cmd)
                
                elif q_type == 'bar_chart':
                    if len(detected_vars) >= 2:
                        # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† Ø§Ù„Ø£ÙˆÙ„ Ù‡Ùˆ Ø§Ù„Ù…ØªØºÙŠØ± Ø§Ù„ÙƒÙ…ÙŠ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ Ù‡Ùˆ Ø§Ù„ÙØ¦ÙˆÙŠ
                        cmd = f"GRAPH /BAR(SIMPLE)=MEAN({detected_vars[0]}) BY {detected_vars[1]}"
                        cmd += f"\n  /TITLE='Average {detected_vars[0]} by {detected_vars[1]}'."
                        if cmd not in processed_commands:
                            code_lines.append(cmd)
                            processed_commands.append(cmd)
                
                elif q_type == 't_test':
                    if len(detected_vars) >= 2:
                        # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† Ø§Ù„Ø£ÙˆÙ„ Ù‡Ùˆ Ù…ØªØºÙŠØ± Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
                        group_var = detected_vars[0]
                        test_vars = ' '.join(detected_vars[1:3])
                        cmd = f"T-TEST GROUPS={group_var}"
                        cmd += f"\n  /VARIABLES={test_vars}"
                        cmd += "\n  /CRITERIA=CI(.95)."
                        if cmd not in processed_commands:
                            code_lines.append(cmd)
                            processed_commands.append(cmd)
                
                elif q_type == 'correlation':
                    if len(detected_vars) >= 2:
                        vars_str = ' '.join(detected_vars[:4])
                        cmd = f"CORRELATIONS /VARIABLES={vars_str}"
                        cmd += "\n  /PRINT=TWOTAIL NOSIG."
                        if cmd not in processed_commands:
                            code_lines.append(cmd)
                            processed_commands.append(cmd)
                
                elif q_type == 'regression':
                    if len(detected_vars) >= 2:
                        dv = detected_vars[0]
                        iv_list = ' '.join(detected_vars[1:4])
                        cmd = f"REGRESSION"
                        cmd += f"\n  /DEPENDENT {dv}"
                        cmd += f"\n  /METHOD=ENTER {iv_list}."
                        if cmd not in processed_commands:
                            code_lines.append(cmd)
                            processed_commands.append(cmd)
                
                elif q_type == 'confidence':
                    for var in detected_vars[:2]:
                        if '95%' in question.lower():
                            cmd = f"EXAMINE VARIABLES={var}"
                            cmd += "\n  /CINTERVAL 95"
                            cmd += "\n  /PLOT NONE."
                        elif '99%' in question.lower():
                            cmd = f"EXAMINE VARIABLES={var}"
                            cmd += "\n  /CINTERVAL 99"
                            cmd += "\n  /PLOT NONE."
                        else:
                            cmd = f"EXAMINE VARIABLES={var}"
                            cmd += "\n  /CINTERVAL 95"
                            cmd += "\n  /PLOT NONE."
                        
                        if cmd not in processed_commands:
                            code_lines.append(cmd)
                            processed_commands.append(cmd)
                
                elif q_type == 'normality':
                    for var in detected_vars[:2]:
                        cmd = f"EXAMINE VARIABLES={var}"
                        cmd += "\n  /PLOT NPPLOT"
                        cmd += "\n  /STATISTICS DESCRIPTIVES."
                        if cmd not in processed_commands:
                            code_lines.append(cmd)
                            processed_commands.append(cmd)
            
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙŠ Ø£ÙˆØ§Ù…Ø±
            if not processed_commands:
                vars_str = ' '.join(detected_vars[:3])
                code_lines.append(f"DESCRIPTIVES VARIABLES={vars_str}")
                code_lines.append("  /STATISTICS=MEAN STDDEV MIN MAX.")
        
        code_lines.append("EXECUTE.")
        code_lines.append("")  # Ø³Ø·Ø± ÙØ§Ø±Øº Ù„ÙØµÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        
        return '\n'.join(code_lines), None
    
    def generate_spss_header(self, df, data_analysis, filename):
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø£Ø³ ÙƒÙˆØ¯ SPSS"""
        header = f"""* =========================================================================
* SPSS SYNTAX FILE
* Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
* Data File: {filename}
* Rows: {data_analysis['summary']['total_rows']}
* Variables: {data_analysis['summary']['total_columns']}
* =========================================================================

* DATA DEFINITION AND SETUP
"""
        
        # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
        var_labels = []
        for var_name, var_info in data_analysis['variables'].items():
            label = var_name.replace('_', ' ').title()
            var_labels.append(f"{var_name} '{label}'")
        
        header += "VARIABLE LABELS\n    " + " /".join(var_labels) + ".\n\n"
        
        # ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
        value_labels = []
        for var_name, var_info in data_analysis['variables'].items():
            if var_info['subtype'] in ['categorical_numeric', 'categorical_text']:
                if var_info.get('values') and len(var_info['values']) <= 10:
                    line = f"    /{var_name} "
                    if var_info['subtype'] == 'categorical_numeric':
                        for val in var_info['values']:
                            line += f"{val} 'Value {val}' "
                    else:
                        for i, val in enumerate(var_info['values'][:5], 1):
                            line += f"{i} '{val}' "
                    value_labels.append(line.strip())
        
        if value_labels:
            header += "VALUE LABELS\n"
            header += "\n".join(value_labels)
            header += ".\n\n"
        
        header += "EXECUTE.\n\n"
        header += "* =========================================================================\n"
        header += "* QUESTION ANALYSIS SECTION\n"
        header += "* =========================================================================\n\n"
        
        return header
    
    def create_download_link(self, content, filename, btn_text="ğŸ“¥ ØªÙ†Ø²ÙŠÙ„"):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø§Ø¨Ø· ØªØ­Ù…ÙŠÙ„"""
        b64 = base64.b64encode(content.encode()).decode()
        return f'<a href="data:file/txt;base64,{b64}" download="{filename}" style="text-decoration: none; padding: 10px 20px; background-color: #4CAF50; color: white; border-radius: 5px; font-weight: bold;">{btn_text} {filename}</a>'

# Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
def main():
    st.sidebar.title("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
    st.sidebar.subheader("Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
    auto_detect_types = st.sidebar.checkbox("Ø§Ù„ÙƒØ´Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª", value=True)
    prevent_duplicates = st.sidebar.checkbox("Ù…Ù†Ø¹ ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª", value=True)
    include_comments = st.sidebar.checkbox("Ø¥Ø¶Ø§ÙØ© ØªØ¹Ù„ÙŠÙ‚Ø§Øª ØªÙˆØ¶ÙŠØ­ÙŠØ©", value=True)
    
    st.sidebar.subheader("ØªØ®ØµÙŠØµ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬")
    output_format = st.sidebar.selectbox(
        "Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ø§ØªØ¬",
        ["SPSS Syntax (.sps)", "Text File (.txt)", "Word Document (.docx)"]
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ„Ø¯
    generator = AdvancedSPSSGenerator()
    
    # Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    col1, col2 = st.columns([3, 1])
    
    with col1:
        st.markdown("### ğŸ“ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª")
    with col2:
        st.markdown("### âš¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø±ÙŠØ¹Ø©")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
    col1, col2 = st.columns(2)
    
    with col1:
        data_file = st.file_uploader(
            "Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Excel/CSV)",
            type=['xlsx', 'xls', 'csv'],
            help="ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ù„Ù Excel Ø£Ùˆ CSV ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
        )
    
    with col2:
        questions_file = st.file_uploader(
            "Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø¦Ù„Ø© (TXT)",
            type=['txt'],
            help="Ù…Ù„Ù Ù†ØµÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ Ø³Ø¤Ø§Ù„ ÙÙŠ ÙƒÙ„ Ø³Ø·Ø±"
        )
    
    if data_file and questions_file:
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if data_file.name.endswith('.csv'):
                df = pd.read_csv(data_file, encoding='utf-8')
            else:
                df = pd.read_excel(data_file)
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            questions_text = questions_file.getvalue().decode('utf-8')
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            data_analysis = generator.analyze_dataset(df)
            
            # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            st.success(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
            
            # Ø¹Ø±Ø¶ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…
            with st.expander("ğŸ“Š Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", expanded=True):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ", data_analysis['summary']['total_rows'])
                with col2:
                    st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª", data_analysis['summary']['total_columns'])
                with col3:
                    st.metric("Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙƒÙ…ÙŠØ©", len(data_analysis['summary']['numeric_vars']))
                
                st.write("**Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª:**")
                for var_name, var_info in list(data_analysis['variables'].items())[:10]:
                    col1, col2, col3 = st.columns([2, 2, 1])
                    with col1:
                        st.write(f"**{var_name}**")
                    with col2:
                        st.write(f"{var_info['type']} ({var_info['subtype']})")
                    with col3:
                        st.write(f"Ù‚ÙŠÙ…: {var_info['unique_values']}")
                
                if len(data_analysis['variables']) > 10:
                    st.write(f"... Ùˆ {len(data_analysis['variables']) - 10} Ù…ØªØºÙŠØ±Ø§Øª Ø£Ø®Ø±Ù‰")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            questions = []
            lines = questions_text.split('\n')
            current_q = ""
            
            for line in lines:
                line = line.strip()
                if re.match(r'^\d+[\.\)]', line) or re.match(r'^\d+\.\s+', line):
                    if current_q:
                        questions.append(current_q.strip())
                    current_q = line
                elif current_q and line and not line.startswith('*'):
                    current_q += " " + line
            
            if current_q:
                questions.append(current_q.strip())
            
            # ØªØµÙÙŠØ© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙØ§Ø±ØºØ©
            questions = [q for q in questions if q and len(q) > 5]
            
            st.info(f"ğŸ“‹ ØªÙ… ØªØ­Ù„ÙŠÙ„ {len(questions)} Ø³Ø¤Ø§Ù„")
            
            # Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            with st.expander("ğŸ‘ï¸ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§"):
                for i, q in enumerate(questions[:10], 1):
                    classifications = generator.classify_question(q)
                    detected_vars = generator.extract_variables_from_text(q, list(df.columns))
                    
                    st.write(f"**Ø§Ù„Ø³Ø¤Ø§Ù„ {i}:**")
                    st.write(f"{q[:150]}{'...' if len(q) > 150 else ''}")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        if classifications:
                            st.caption(f"**Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª:** {', '.join(classifications)}")
                    with col2:
                        if detected_vars:
                            st.caption(f"**Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª:** {', '.join(detected_vars)}")
                    
                    st.write("---")
                
                if len(questions) > 10:
                    st.write(f"... Ùˆ {len(questions) - 10} Ø£Ø³Ø¦Ù„Ø© Ø£Ø®Ø±Ù‰")
            
            # Ø²Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            st.markdown("---")
            if st.button("ğŸš€ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯", type="primary", use_container_width=True):
                
                with st.spinner(f"Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© {len(questions)} Ø³Ø¤Ø§Ù„..."):
                    
                    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø£Ø³
                    spss_code = generator.generate_spss_header(df, data_analysis, data_file.name)
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø³Ø¤Ø§Ù„
                    skipped_questions = []
                    question_stats = {
                        'total': len(questions),
                        'processed': 0,
                        'skipped': 0
                    }
                    
                    progress_bar = st.progress(0)
                    
                    for i, question in enumerate(questions, 1):
                        # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
                        progress_bar.progress(i / len(questions))
                        
                        # ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ Ù„Ù„Ø³Ø¤Ø§Ù„
                        question_code, skip_reason = generator.generate_spss_for_question(
                            i, question, df, data_analysis
                        )
                        
                        if question_code:
                            spss_code += question_code
                            question_stats['processed'] += 1
                        else:
                            skipped_questions.append({
                                'number': i,
                                'question': question[:100],
                                'reason': skip_reason
                            })
                            question_stats['skipped'] += 1
                    
                    # Ø¥Ø¶Ø§ÙØ© ØªØ°ÙŠÙŠÙ„
                    spss_code += f"""* =========================================================================
* END OF ANALYSIS
* Total Questions Processed: {question_stats['processed']}
* Duplicate Questions Skipped: {question_stats['skipped']}
* Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
* =========================================================================
"""
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                    st.success(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {question_stats['processed']} Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­")
                    
                    if skipped_questions:
                        st.warning(f"âš ï¸ ØªÙ… ØªØ®Ø·ÙŠ {question_stats['skipped']} Ø³Ø¤Ø§Ù„ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±")
                        with st.expander("Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ®Ø·Ø§Ø©"):
                            for skipped in skipped_questions:
                                st.write(f"**Ø§Ù„Ø³Ø¤Ø§Ù„ {skipped['number']}:** {skipped['question']}")
                                st.caption(f"Ø§Ù„Ø³Ø¨Ø¨: {skipped['reason']}")
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ø§ØªØ¬
                    st.subheader("ğŸ“‹ ÙƒÙˆØ¯ SPSS Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
                    
                    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¶
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        show_full = st.checkbox("Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù…Ù„Ø§Ù‹", value=False)
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙˆØ¯
                    if show_full:
                        st.code(spss_code, language='text', height=600)
                    else:
                        code_lines = spss_code.split('\n')
                        st.code('\n'.join(code_lines[:200]), language='text')
                        if len(code_lines) > 200:
                            st.info(f"ÙŠØªÙ… Ø¹Ø±Ø¶ 200 Ø³Ø·Ø± Ù…Ù† Ø£ØµÙ„ {len(code_lines)}. Ù‚Ù… Ø¨ØªÙØ¹ÙŠÙ„ 'Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù…Ù„Ø§Ù‹' Ù„Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù…Ù„Ø§Ù‹.")
                    
                    # Ù‚Ø³Ù… Ø§Ù„ØªÙ†Ø²ÙŠÙ„
                    st.markdown("---")
                    st.subheader("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.markdown(generator.create_download_link(
                            spss_code, "SPSS_Analysis.sps", "ğŸ“Š"
                        ), unsafe_allow_html=True)
                    
                    with col2:
                        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªÙ‚Ø±ÙŠØ±
                        report = f"""ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ SPSS
ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Ø§Ù„Ù…Ù„Ù: {data_file.name}
Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©: {len(questions)}
Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {question_stats['processed']}
Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ®Ø·Ø§Ø©: {question_stats['skipped']}

ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:
"""
                        for i, (fingerprint, info) in enumerate(generator.processed_questions.items(), 1):
                            report += f"""
{i}. Ø§Ù„Ø³Ø¤Ø§Ù„ {info['number']}:
   - Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {info['question']}
   - Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª: {', '.join(info['variables'])}
   - Ø§Ù„Ø£Ù†ÙˆØ§Ø¹: {', '.join(info['types'])}
"""
                        
                        st.markdown(generator.create_download_link(
                            report, "Analysis_Report.txt", "ğŸ“„"
                        ), unsafe_allow_html=True)
                    
                    with col3:
                        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªØ¹Ù„ÙŠÙ…Ø§Øª
                        instructions = f"""ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙˆØ¯ SPSS:
1. Ø§ÙØªØ­ Ø¨Ø±Ù†Ø§Ù…Ø¬ SPSS
2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {data_file.name}
3. Ø§ÙØªØ­ Ù…Ø­Ø±Ø± Ø§Ù„ØµÙŠØº (Syntax Editor)
4. Ø§Ù„ØµÙ‚ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø±ÙÙ‚
5. Ø­Ø¯Ø¯ Ø§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù…Ù„Ø§Ù‹ (Ctrl+A)
6. Ø§Ø¶ØºØ· F5 Ø£Ùˆ Ø§Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„ØªØ´ØºÙŠÙ„
7. Ø§ÙØ­Øµ Ù†Ø§ÙØ°Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (Output) Ù„Ù„Ù†ØªØ§Ø¦Ø¬

Ù…Ù„Ø§Ø­Ø¸Ø§Øª:
- Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¹Ø§Ù„Ø¬ {question_stats['processed']} Ø³Ø¤Ø§Ù„
- ØªÙ… ØªØ¬Ù†Ø¨ ØªÙƒØ±Ø§Ø± {question_stats['skipped']} Ø³Ø¤Ø§Ù„
- ÙƒÙ„ Ø³Ø¤Ø§Ù„ Ù„Ù‡ ØªØ­Ù„ÙŠÙ„ ÙØ±ÙŠØ¯ ÙˆÙ…Ø®ØªÙ„Ù
"""
                        
                        st.markdown(generator.create_download_link(
                            instructions, "Instructions.txt", "ğŸ“"
                        ), unsafe_allow_html=True)
                    
                    # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
                    with st.expander("ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„"):
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©", question_stats['processed'])
                        with col2:
                            st.metric("Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©", len(df.columns))
                        with col3:
                            st.metric("Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª", len(set([
                                t for q in generator.processed_questions.values() 
                                for t in q['types']
                            ])))
                        
                        # Ø¹Ø±Ø¶ ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
                        type_counts = {}
                        for q_info in generator.processed_questions.values():
                            for q_type in q_info['types']:
                                type_counts[q_type] = type_counts.get(q_type, 0) + 1
                        
                        st.write("**ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª:**")
                        for t_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
                            st.write(f"- {t_type}: {count} Ø³Ø¤Ø§Ù„")
        
        except Exception as e:
            st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
            st.exception(e)
    
    else:
        # ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨
        st.info("""
        ## ğŸ¯ Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù…ÙˆÙ„Ø¯ Ø£ÙƒÙˆØ§Ø¯ SPSS Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
        
        **Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:**
        âœ… **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©** - ÙƒÙ„ Ø³Ø¤Ø§Ù„ ÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ ÙØ±ÙŠØ¯
        âœ… **Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±** - Ù†Ø¸Ø§Ù… Ø¨ØµÙ…Ø§Øª ÙŠÙ…Ù†Ø¹ ØªÙƒØ±Ø§Ø± Ù†ÙØ³ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        âœ… **ØªØ­Ù„ÙŠÙ„ Ø°ÙƒÙŠ** - Ø§ÙƒØªØ´Ø§Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ø£Ù†ÙˆØ§Ø¹
        âœ… **Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø©** - ØªÙ‚Ø§Ø±ÙŠØ± Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„Ù…ØªØ®Ø·Ø§Ø©
        
        **ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
        1. **Ø­Ù…Ù‘Ù„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª** (Excel Ø£Ùˆ CSV)
        2. **Ø­Ù…Ù‘Ù„ Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø¦Ù„Ø©** (Ù…Ù„Ù Ù†ØµÙŠ)
        3. **Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©**
        4. **Ø­Ù…Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø©**
        
        **Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹:**
        - Ø£ÙŠ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        - Ø£ÙŠ ØªÙ†Ø³ÙŠÙ‚ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        - Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©
        - Ø§ÙƒØªØ´Ø§Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© ÙÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        """)
        
        # Ø£Ù…Ø«Ù„Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ©
        with st.expander("ğŸ“š Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"):
            st.markdown("""
            ### Ù…Ø«Ø§Ù„ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø©:
            ```
            1. Ø§Ø­Ø³Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· ÙˆØ§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù„Ù„Ø¹Ù…Ø± ÙˆØ§Ù„Ø¯Ø®Ù„
            2. Ø£Ù†Ø´Ø¦ Ø¬Ø¯ÙˆÙ„Ø§Ù‹ ØªÙƒØ±Ø§Ø±ÙŠØ§Ù‹ Ù„Ù„Ù†ÙˆØ¹ ÙˆØ§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ
            3. Ø§Ø±Ø³Ù… Ù…Ø®Ø·Ø·Ø§Ù‹ Ø´Ø±ÙŠØ·ÙŠØ§Ù‹ ÙŠÙˆØ¶Ø­ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ø®Ù„ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            4. Ø§Ø®ØªØ¨Ø± Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ÙØ±Ù‚ ÙÙŠ Ø§Ù„Ø¹Ù…Ø± Ø¨ÙŠÙ† Ø§Ù„Ø°ÙƒÙˆØ± ÙˆØ§Ù„Ø¥Ù†Ø§Ø«
            5. Ø§ÙØ­Øµ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø© ÙˆØ§Ù„Ø¯Ø®Ù„
            6. Ø§Ø±Ø³Ù… Ù…Ø®Ø·Ø·Ø§Ù‹ Ù…Ø¨Ø¹Ø«Ø±Ø§Ù‹ Ù„Ù„Ø¹Ù…Ø± Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¯Ø®Ù„
            7. Ø£Ù†Ø´Ø¦ ÙØªØ±Ø§Øª Ø«Ù‚Ø© 95% Ù„Ù„Ø¯Ø®Ù„
            8. Ø§ÙØ­Øµ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ù…Ø± Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ
            9. Ø§Ø­Ø³Ø¨ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÙŠÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙƒÙ…ÙŠØ©
            10. Ø­Ù„Ù„ ØªØ£Ø«ÙŠØ± Ø§Ù„Ù†ÙˆØ¹ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø®Ù„
            ```
            
            **Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©:** ÙƒÙ„ Ø³Ø¤Ø§Ù„ Ø³ÙˆÙ ÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ ÙØ±ÙŠØ¯ ÙˆÙ…Ø®ØªÙ„Ù!
            """)
        
        # Ø²Ø± ØªØ¬Ø±ÙŠØ¨ÙŠ
        if st.button("ğŸ”„ ØªØ´ØºÙŠÙ„ Ù…Ø«Ø§Ù„ ØªØ¬Ø±ÙŠØ¨ÙŠ", type="secondary"):
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            np.random.seed(42)
            sample_data = pd.DataFrame({
                'Ø§Ù„Ø¹Ù…Ø±': np.random.randint(20, 60, 50),
                'Ø§Ù„Ø¯Ø®Ù„': np.random.normal(5000, 1500, 50),
                'Ø§Ù„Ù†ÙˆØ¹': np.random.choice(['Ø°ÙƒØ±', 'Ø£Ù†Ø«Ù‰'], 50),
                'Ø§Ù„Ù…Ø³ØªÙˆÙ‰_Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ': np.random.choice(['Ø«Ø§Ù†ÙˆÙŠ', 'Ø¨ÙƒØ§Ù„ÙˆØ±ÙŠÙˆØ³', 'Ù…Ø§Ø¬Ø³ØªÙŠØ±', 'Ø¯ÙƒØªÙˆØ±Ø§Ù‡'], 50),
                'Ø³Ù†ÙˆØ§Øª_Ø§Ù„Ø®Ø¨Ø±Ø©': np.random.randint(1, 30, 50),
                'Ø§Ù„Ù‚Ø³Ù…': np.random.choice(['Ù…Ø¨ÙŠØ¹Ø§Øª', 'ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§', 'Ù…ÙˆØ§Ø±Ø¯ Ø¨Ø´Ø±ÙŠØ©', 'Ù…Ø§Ù„ÙŠØ©'], 50)
            })
            
            sample_questions = """1. Ø§Ø­Ø³Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„Ø¹Ù…Ø± ÙˆØ§Ù„Ø¯Ø®Ù„
2. Ø£Ù†Ø´Ø¦ Ø¬Ø¯Ø§ÙˆÙ„ ØªÙƒØ±Ø§Ø±ÙŠØ© Ù„Ù„Ù†ÙˆØ¹ ÙˆØ§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ
3. Ø§Ø±Ø³Ù… Ù…Ø®Ø·Ø·Ø§Ù‹ Ø´Ø±ÙŠØ·ÙŠØ§Ù‹ Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ø®Ù„ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
4. Ø§Ø®ØªØ¨Ø± ÙØ±Ù‚ Ø§Ù„Ø¹Ù…Ø± Ø¨ÙŠÙ† Ø§Ù„Ø°ÙƒÙˆØ± ÙˆØ§Ù„Ø¥Ù†Ø§Ø«
5. Ø§ÙØ­Øµ Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù…Ø± ÙˆØ§Ù„Ø¯Ø®Ù„
6. Ø§Ø±Ø³Ù… Ù…Ø®Ø·Ø·Ø§Ù‹ Ù…Ø¨Ø¹Ø«Ø±Ø§Ù‹ Ù„Ù„Ø¹Ù…Ø± Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¯Ø®Ù„
7. Ø§Ø­Ø³Ø¨ ÙØªØ±Ø§Øª Ø«Ù‚Ø© 95% Ù„Ù„Ø¯Ø®Ù„
8. Ø§ÙØ­Øµ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ù…Ø±
9. Ø­Ù„Ù„ ØªØ£Ø«ÙŠØ± Ø§Ù„Ù†ÙˆØ¹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø®Ù„
10. Ø§Ø±Ø³Ù… Ù…Ø®Ø·Ø·Ø§Ù‹ ØµÙ†Ø¯ÙˆÙ‚ÙŠØ§Ù‹ Ù„Ù„Ø¯Ø®Ù„ Ø­Ø³Ø¨ Ø§Ù„Ù‚Ø³Ù…"""
            
            st.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ù†Ø¬Ø§Ø­!")
            st.write("**Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©:**")
            st.dataframe(sample_data)
            st.write("**Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©:**")
            st.text(sample_questions)

if __name__ == "__main__":
    main()
