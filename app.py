import streamlit as st
import pandas as pd
import numpy as np
import re
import base64
from datetime import datetime
from collections import OrderedDict
import hashlib

# Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙØ­Ø© Streamlit
st.set_page_config(
    page_title="SPSS Code Generator - Intelligent Version",
    page_icon="ğŸ“Š",
    layout="wide"
)

st.title("ğŸ“Š Ù…ÙˆÙ„Ø¯ Ø£ÙƒÙˆØ§Ø¯ SPSS Ø§Ù„Ø°ÙƒÙŠ")
st.markdown("### ÙŠØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆÙŠÙˆÙ„Ø¯ Ø£ÙƒÙˆØ§Ø¯ Ø¯Ù‚ÙŠÙ‚Ø© Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„")

class IntelligentSPSSGenerator:
    def __init__(self):
        self.processed_questions = OrderedDict()
        self.variable_mapping = self._create_variable_mapping()
        self.question_types = {
            'descriptive': ['mean', 'average', 'median', 'mode', 'standard deviation', 'variance', 'descriptive', 'calculate', 'compute'],
            'frequency': ['frequency', 'distribution', 'count', 'table', 'percentage', 'percent', 'proportion'],
            't_test': ['t-test', 't test', 'compare means', 'independent samples', 'paired'],
            'anova': ['anova', 'analysis of variance', 'f-test', 'one-way', 'two-way'],
            'correlation': ['correlation', 'relationship', 'association', 'correlate', 'relationship between'],
            'regression': ['regression', 'predict', 'linear model', 'multiple regression'],
            'chi_square': ['chi-square', 'chi squared', 'contingency', 'association categorical'],
            'graph': ['graph', 'chart', 'histogram', 'bar chart', 'pie chart', 'scatter', 'plot', 'draw'],
            'confidence': ['confidence interval', 'ci', '95%', '99%', 'interval', 'construct the confidence'],
            'normality': ['normality', 'normal distribution', 'shapiro-wilk', 'kolmogorov', 'empirical rule', 'chebyshev'],
            'outliers': ['outliers', 'extreme values', 'unusual observations', 'extreme value', 'determine the outliers'],
            'group_comparison': ['by group', 'for each', 'compare groups', 'between groups', 'for each city', 'by city'],
            'recode': ['recode', 'categorize', 'group into', 'create classes', 'classify', 'suitable number of classes'],
            'transform': ['transform', 'compute', 'create variable', 'new variable', 'calculate']
        }
    
    def _create_variable_mapping(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ Ù„Ø±Ø¨Ø· Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©"""
        return {
            # Ù…ØµØ·Ù„Ø­Ø§Øª Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©
            'account': ['X1', 'balance', 'account_balance'],
            'balance': ['X1', 'account_balance'],
            'atm': ['X2', 'transactions', 'atm_transactions'],
            'transaction': ['X2', 'atm', 'transactions'],
            'service': ['X3', 'other_services', 'services'],
            'debit': ['X4', 'debit_card', 'card'],
            'card': ['X4', 'debit_card'],
            'interest': ['X5', 'interest_received'],
            'city': ['X6', 'location', 'city_location'],
            'location': ['X6', 'city'],
            
            # Ù…ØµØ·Ù„Ø­Ø§Øª Ø¹Ø§Ù…Ø©
            'income': ['X1', 'salary', 'revenue'],
            'salary': ['X1', 'income'],
            'age': ['age_var'],
            'gender': ['gender_var', 'sex'],
            'education': ['edu_var', 'education_level'],
            'score': ['score_var', 'test_score'],
            'price': ['price_var', 'cost'],
            'quantity': ['quantity_var', 'amount'],
            'rate': ['rate_var', 'percentage'],
            'category': ['cat_var', 'group']
        }
    
    def analyze_dataset(self, df):
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ù†Ø´Ø§Ø¡ ØªØ³Ù…ÙŠØ§Øª Ø°ÙƒÙŠØ©"""
        analysis = {
            'variables': {},
            'summary': {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'numeric_vars': [],
                'categorical_vars': [],
                'text_vars': [],
                'column_names': list(df.columns)
            },
            'suggested_labels': {}
        }
        
        for column in df.columns:
            col_data = df[column]
            var_info = {
                'name': column,
                'type': 'unknown',
                'missing': int(col_data.isna().sum()),
                'missing_percent': round(col_data.isna().sum() / len(df) * 100, 2),
                'unique_values': int(col_data.nunique()),
                'values': []
            }
            
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù‚Ù…
                numeric_data = pd.to_numeric(col_data.dropna())
                var_info['type'] = 'numeric'
                var_info['min'] = float(numeric_data.min())
                var_info['max'] = float(numeric_data.max())
                var_info['mean'] = float(numeric_data.mean())
                var_info['std'] = float(numeric_data.std())
                var_info['median'] = float(numeric_data.median())
                
                if var_info['unique_values'] <= 10:
                    var_info['subtype'] = 'categorical_numeric'
                    var_info['values'] = sorted([float(x) for x in numeric_data.unique()])
                    analysis['summary']['categorical_vars'].append(column)
                else:
                    var_info['subtype'] = 'continuous'
                    analysis['summary']['numeric_vars'].append(column)
                    
            except:
                # Ù…ØªØºÙŠØ± Ù†ØµÙŠ
                var_info['type'] = 'text'
                unique_vals = list(col_data.dropna().unique())
                var_info['values'] = unique_vals[:10]
                
                if var_info['unique_values'] <= 15:
                    var_info['subtype'] = 'categorical_text'
                    analysis['summary']['categorical_vars'].append(column)
                else:
                    var_info['subtype'] = 'free_text'
                    analysis['summary']['text_vars'].append(column)
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªØ³Ù…ÙŠØ© Ù…Ù‚ØªØ±Ø­Ø© Ù„Ù„Ù…ØªØºÙŠØ±
            suggested_label = self._suggest_variable_label(column, var_info)
            analysis['suggested_labels'][column] = suggested_label
            
            analysis['variables'][column] = var_info
        
        return analysis
    
    def _suggest_variable_label(self, column_name, var_info):
        """Ø§Ù‚ØªØ±Ø§Ø­ ØªØ³Ù…ÙŠØ© Ø°ÙƒÙŠØ© Ù„Ù„Ù…ØªØºÙŠØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ø³Ù…Ù‡ ÙˆØ®ØµØ§Ø¦ØµÙ‡"""
        column_lower = column_name.lower()
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ØªØºÙŠØ± Ù„Ù‡ Ø§Ø³Ù… Ø´Ø§Ø¦Ø¹
        if 'x1' in column_lower or column_name == 'X1':
            return 'Account Balance ($)'
        elif 'x2' in column_lower or column_name == 'X2':
            return 'ATM Transactions'
        elif 'x3' in column_lower or column_name == 'X3':
            return 'Other Services'
        elif 'x4' in column_lower or column_name == 'X4':
            return 'Debit Card Holder'
        elif 'x5' in column_lower or column_name == 'X5':
            return 'Interest Received'
        elif 'x6' in column_lower or column_name == 'X6':
            return 'City Location'
        
        # Ø§Ù‚ØªØ±Ø§Ø­ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        if var_info['subtype'] == 'continuous':
            if var_info['mean'] > 1000:
                return f'{column_name} (Large Values)'
            else:
                return f'{column_name} (Continuous)'
        elif var_info['subtype'] == 'categorical_numeric':
            if var_info['unique_values'] == 2:
                return f'{column_name} (Binary: 0/1)'
            else:
                return f'{column_name} (Categorical)'
        elif var_info['subtype'] == 'categorical_text':
            return f'{column_name} (Categories)'
        
        return column_name.replace('_', ' ').title()
    
    def detect_variables_in_question(self, question, df_columns, data_analysis):
        """ÙƒØ´Ù Ø°ÙƒÙŠ Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„"""
        question_lower = question.lower()
        detected_vars = []
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù† Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        for column in df_columns:
            col_lower = column.lower()
            if (col_lower in question_lower or 
                f' {col_lower} ' in f' {question_lower} ' or
                question_lower.startswith(col_lower) or
                question_lower.endswith(col_lower)):
                detected_vars.append(column)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø±Ø¨Ø·
        for keyword, possible_vars in self.variable_mapping.items():
            if keyword in question_lower:
                for possible_var in possible_vars:
                    if possible_var in df_columns and possible_var not in detected_vars:
                        detected_vars.append(possible_var)
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ØµØ·Ù„Ø­Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©
        key_terms = {
            'account balance': ['X1'],
            'balance': ['X1'],
            'atm transaction': ['X2'],
            'transaction': ['X2'],
            'debit card': ['X4'],
            'interest': ['X5'],
            'city': ['X6'],
            'location': ['X6'],
            'mean': data_analysis['summary']['numeric_vars'][:2] if data_analysis['summary']['numeric_vars'] else [],
            'average': data_analysis['summary']['numeric_vars'][:2] if data_analysis['summary']['numeric_vars'] else [],
            'frequency': data_analysis['summary']['categorical_vars'][:3] if data_analysis['summary']['categorical_vars'] else [],
            'histogram': data_analysis['summary']['numeric_vars'][:2] if data_analysis['summary']['numeric_vars'] else [],
            'bar chart': data_analysis['summary']['categorical_vars'][:1] + data_analysis['summary']['numeric_vars'][:1] 
                         if data_analysis['summary']['categorical_vars'] and data_analysis['summary']['numeric_vars'] else [],
            'pie chart': data_analysis['summary']['categorical_vars'][:1] if data_analysis['summary']['categorical_vars'] else [],
            'confidence interval': data_analysis['summary']['numeric_vars'][:1] if data_analysis['summary']['numeric_vars'] else []
        }
        
        for term, vars_list in key_terms.items():
            if term in question_lower and vars_list:
                for var in vars_list:
                    if var not in detected_vars:
                        detected_vars.append(var)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        return list(OrderedDict.fromkeys(detected_vars))
    
    def classify_question(self, question):
        """ØªØµÙ†ÙŠÙ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ù…ØªØ·Ù„Ø¨Ø§ØªÙ‡"""
        question_lower = question.lower()
        classifications = []
        
        for q_type, keywords in self.question_types.items():
            for keyword in keywords:
                if re.search(r'\b' + re.escape(keyword) + r'\b', question_lower):
                    classifications.append(q_type)
                    break
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
        if 'frequency' in classifications and 'table' in question_lower:
            classifications.append('frequency_table')
        
        if 'graph' in classifications:
            if 'histogram' in question_lower:
                classifications.append('histogram')
            if 'bar' in question_lower and 'chart' in question_lower:
                classifications.append('bar_chart')
            if 'pie' in question_lower and 'chart' in question_lower:
                classifications.append('pie_chart')
            if 'scatter' in question_lower:
                classifications.append('scatter_plot')
        
        if 'confidence' in classifications:
            if '95%' in question_lower:
                classifications.append('ci_95')
            if '99%' in question_lower:
                classifications.append('ci_99')
        
        if not classifications:
            classifications.append('general_analysis')
        
        return list(OrderedDict.fromkeys(classifications))
    
    def generate_spss_for_question(self, q_num, question, df, data_analysis):
        """ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ SPSS Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…Ø®ØµØµ Ù„Ù„Ø³Ø¤Ø§Ù„"""
        
        # ÙƒØ´Ù Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
        detected_vars = self.detect_variables_in_question(
            question, 
            data_analysis['summary']['column_names'],
            data_analysis
        )
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø¤Ø§Ù„
        classifications = self.classify_question(question)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ØµÙ…Ø© ÙØ±ÙŠØ¯Ø©
        fingerprint = self._create_question_fingerprint(question, detected_vars, classifications)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±
        if fingerprint in self.processed_questions:
            similar_q = self.processed_questions[fingerprint]
            return None, f"ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„ Ù…Ø´Ø§Ø¨Ù‡ (Ø§Ù„Ø³Ø¤Ø§Ù„ {similar_q['number']})"
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ØµÙ…Ø©
        self.processed_questions[fingerprint] = {
            'number': q_num,
            'question': question[:100],
            'variables': detected_vars,
            'types': classifications,
            'fingerprint': fingerprint
        }
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯
        code_lines = []
        code_lines.append(f"* {'='*70}")
        code_lines.append(f"* QUESTION {q_num}: {question[:80]}{'...' if len(question) > 80 else ''}")
        code_lines.append(f"* Classification: {', '.join(classifications)}")
        
        if detected_vars:
            var_labels = [data_analysis['suggested_labels'].get(v, v) for v in detected_vars]
            code_lines.append(f"* Variables: {', '.join([f'{v} ({l})' for v, l in zip(detected_vars[:3], var_labels[:3])])}")
            if len(detected_vars) > 3:
                code_lines.append(f"* ... and {len(detected_vars) - 3} more variables")
        
        code_lines.append(f"* {'='*70}\n")
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
        analysis_code = self._generate_specific_analysis(
            q_num, question, detected_vars, classifications, data_analysis
        )
        
        code_lines.append(analysis_code)
        code_lines.append("EXECUTE.")
        code_lines.append("")
        
        return '\n'.join(code_lines), None
    
    def _create_question_fingerprint(self, question, detected_vars, classifications):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ØµÙ…Ø© ÙØ±ÙŠØ¯Ø© Ù„Ù„Ø³Ø¤Ø§Ù„"""
        # ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        simple_question = re.sub(r'\d+', '#', question.lower())
        simple_question = re.sub(r'\s+', ' ', simple_question).strip()
        
        components = [
            '|'.join(sorted(classifications)),
            '|'.join(sorted(detected_vars)),
            simple_question[:50]
        ]
        
        fingerprint_string = '@@@'.join(components)
        return hashlib.md5(fingerprint_string.encode()).hexdigest()[:10]
    
    def _generate_specific_analysis(self, q_num, question, detected_vars, classifications, data_analysis):
        """ØªÙˆÙ„ÙŠØ¯ ØªØ­Ù„ÙŠÙ„ Ù…Ø­Ø¯Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„"""
        question_lower = question.lower()
        analysis_lines = []
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…ØªØºÙŠØ±Ø§ØªØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØºÙŠØ±Ø§Øª Ø°ÙƒÙŠØ©
        if not detected_vars:
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ®Ù…ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
            if 'account balance' in question_lower:
                detected_vars = ['X1']
            elif 'atm' in question_lower or 'transaction' in question_lower:
                detected_vars = ['X2']
            elif 'debit card' in question_lower:
                detected_vars = ['X4']
            elif 'interest' in question_lower:
                detected_vars = ['X5']
            elif 'city' in question_lower:
                detected_vars = ['X6']
            else:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØºÙŠØ±Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
                if 'frequency' in classifications:
                    detected_vars = data_analysis['summary']['categorical_vars'][:3]
                elif 'descriptive' in classifications:
                    detected_vars = data_analysis['summary']['numeric_vars'][:2]
                elif 'histogram' in classifications:
                    detected_vars = data_analysis['summary']['numeric_vars'][:2]
                else:
                    detected_vars = data_analysis['summary']['column_names'][:3]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
        for q_type in classifications:
            if q_type == 'descriptive':
                if detected_vars:
                    vars_str = ' '.join(detected_vars[:3])
                    analysis_lines.append(f"* Descriptive statistics")
                    analysis_lines.append(f"FREQUENCIES VARIABLES={vars_str}")
                    analysis_lines.append("  /FORMAT=NOTABLE")
                    analysis_lines.append("  /STATISTICS=MEAN MEDIAN MODE MINIMUM MAXIMUM RANGE VARIANCE STDDEV SKEWNESS SESKEW.")
                    analysis_lines.append("")
            
            elif q_type == 'frequency_table':
                categorical_vars = [v for v in detected_vars 
                                  if data_analysis['variables'].get(v, {}).get('subtype') in 
                                  ['categorical_numeric', 'categorical_text']]
                
                if not categorical_vars:
                    categorical_vars = data_analysis['summary']['categorical_vars'][:3]
                
                if categorical_vars:
                    vars_str = ' '.join(categorical_vars[:3])
                    analysis_lines.append(f"* Frequency tables")
                    analysis_lines.append(f"FREQUENCIES VARIABLES={vars_str}")
                    analysis_lines.append("  /ORDER=ANALYSIS")
                    analysis_lines.append("  /BARCHART.")
                    analysis_lines.append("")
            
            elif q_type == 'histogram':
                numeric_vars = [v for v in detected_vars 
                              if data_analysis['variables'].get(v, {}).get('subtype') == 'continuous']
                
                if not numeric_vars:
                    numeric_vars = data_analysis['summary']['numeric_vars'][:2]
                
                for var in numeric_vars[:2]:
                    analysis_lines.append(f"* Histogram for {var}")
                    analysis_lines.append(f"GRAPH /HISTOGRAM={var}")
                    analysis_lines.append(f"  /TITLE='Histogram of {data_analysis['suggested_labels'].get(var, var)}'.")
                    analysis_lines.append("")
            
            elif q_type == 'bar_chart':
                if len(detected_vars) >= 2:
                    # Ø§ÙØªØ±Ø§Ø¶ Ø£Ù† Ø§Ù„Ø£ÙˆÙ„ ÙƒÙ…ÙŠ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ ÙØ¦ÙˆÙŠ
                    analysis_lines.append(f"* Bar chart")
                    analysis_lines.append(f"GRAPH /BAR(SIMPLE)=MEAN({detected_vars[0]}) BY {detected_vars[1]}")
                    analysis_lines.append(f"  /TITLE='Average {data_analysis['suggested_labels'].get(detected_vars[0], detected_vars[0])} by {data_analysis['suggested_labels'].get(detected_vars[1], detected_vars[1])}'.")
                    analysis_lines.append("")
                elif detected_vars:
                    # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØºÙŠØ± ÙØ¦ÙˆÙŠ Ù„Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ
                    categorical_vars = [v for v in detected_vars 
                                      if data_analysis['variables'].get(v, {}).get('subtype') in 
                                      ['categorical_numeric', 'categorical_text']]
                    if categorical_vars:
                        var = categorical_vars[0]
                        analysis_lines.append(f"* Bar chart for {var}")
                        analysis_lines.append(f"GRAPH /BAR(SIMPLE)=PCT BY {var}")
                        analysis_lines.append(f"  /TITLE='Percentage Distribution of {data_analysis['suggested_labels'].get(var, var)}'.")
                        analysis_lines.append("")
            
            elif q_type == 'pie_chart':
                categorical_vars = [v for v in detected_vars 
                                  if data_analysis['variables'].get(v, {}).get('subtype') in 
                                  ['categorical_numeric', 'categorical_text']]
                
                if categorical_vars:
                    var = categorical_vars[0]
                    analysis_lines.append(f"* Pie chart for {var}")
                    analysis_lines.append(f"GRAPH /PIE=PCT BY {var}")
                    analysis_lines.append(f"  /TITLE='Pie Chart: {data_analysis['suggested_labels'].get(var, var)}'.")
                    analysis_lines.append("")
            
            elif q_type == 'group_comparison':
                if 'city' in question_lower and detected_vars:
                    group_var = 'X6' if 'X6' in data_analysis['summary']['column_names'] else detected_vars[0]
                    analysis_vars = [v for v in detected_vars if v != group_var][:2]
                    
                    if analysis_vars:
                        vars_str = ' '.join(analysis_vars)
                        analysis_lines.append(f"* Analysis by {group_var}")
                        analysis_lines.append(f"SORT CASES BY {group_var}.")
                        analysis_lines.append(f"SPLIT FILE LAYERED BY {group_var}.")
                        analysis_lines.append(f"FREQUENCIES VARIABLES={vars_str}")
                        analysis_lines.append("  /FORMAT=NOTABLE")
                        analysis_lines.append("  /STATISTICS=MEAN MEDIAN MODE MIN MAX.")
                        analysis_lines.append("SPLIT FILE OFF.")
                        analysis_lines.append("")
            
            elif q_type in ['ci_95', 'ci_99', 'confidence']:
                numeric_vars = [v for v in detected_vars 
                              if data_analysis['variables'].get(v, {}).get('subtype') == 'continuous']
                
                if not numeric_vars:
                    numeric_vars = data_analysis['summary']['numeric_vars'][:1]
                
                for var in numeric_vars[:1]:
                    if '99%' in question_lower or q_type == 'ci_99':
                        analysis_lines.append(f"* 99% Confidence Interval for {var}")
                        analysis_lines.append(f"EXAMINE VARIABLES={var}")
                        analysis_lines.append("  /STATISTICS DESCRIPTIVES")
                        analysis_lines.append("  /CINTERVAL 99")
                        analysis_lines.append("  /PLOT NONE.")
                    else:
                        analysis_lines.append(f"* 95% Confidence Interval for {var}")
                        analysis_lines.append(f"EXAMINE VARIABLES={var}")
                        analysis_lines.append("  /STATISTICS DESCRIPTIVES")
                        analysis_lines.append("  /CINTERVAL 95")
                        analysis_lines.append("  /PLOT NONE.")
                    analysis_lines.append("")
            
            elif q_type == 'normality':
                numeric_vars = [v for v in detected_vars 
                              if data_analysis['variables'].get(v, {}).get('subtype') == 'continuous']
                
                if not numeric_vars:
                    numeric_vars = data_analysis['summary']['numeric_vars'][:1]
                
                for var in numeric_vars[:1]:
                    analysis_lines.append(f"* Normality test for {var}")
                    analysis_lines.append(f"EXAMINE VARIABLES={var}")
                    analysis_lines.append("  /PLOT NPPLOT")
                    analysis_lines.append("  /STATISTICS DESCRIPTIVES.")
                    analysis_lines.append("ECHO 'Check Shapiro-Wilk test: If Sig. > 0.05, data is normal (use Empirical Rule).'.")
                    analysis_lines.append("ECHO 'If Sig. < 0.05, data is not normal (use Chebyshev Rule).'.")
                    analysis_lines.append("")
            
            elif q_type == 'outliers':
                numeric_vars = [v for v in detected_vars 
                              if data_analysis['variables'].get(v, {}).get('subtype') == 'continuous']
                
                if not numeric_vars:
                    numeric_vars = data_analysis['summary']['numeric_vars'][:1]
                
                for var in numeric_vars[:1]:
                    analysis_lines.append(f"* Outlier detection for {var}")
                    analysis_lines.append(f"EXAMINE VARIABLES={var}")
                    analysis_lines.append("  /PLOT BOXPLOT")
                    analysis_lines.append("  /STATISTICS DESCRIPTIVES.")
                    analysis_lines.append("ECHO 'Outliers are points beyond the whiskers in the boxplot.'.")
                    analysis_lines.append("ECHO 'Extreme values are marked with * or o in the output.'.")
                    analysis_lines.append("")
            
            elif q_type == 'recode':
                if 'account balance' in question_lower or 'X1' in detected_vars:
                    analysis_lines.append(f"* Recoding Account Balance (X1) into classes")
                    analysis_lines.append(f"RECODE X1 (0 thru 500=1) (500.01 thru 1000=2) (1000.01 thru 1500=3) (1500.01 thru 2000=4) (2000.01 thru HI=5) INTO X1_Classes.")
                    analysis_lines.append(f"VALUE LABELS X1_Classes 1 '0-500' 2 '501-1000' 3 '1001-1500' 4 '1501-2000' 5 'Over 2000'.")
                    analysis_lines.append(f"FREQUENCIES VARIABLES=X1_Classes /FORMAT=AVALUE.")
                    analysis_lines.append("")
                
                elif 'atm' in question_lower or 'transaction' in question_lower or 'X2' in detected_vars:
                    analysis_lines.append(f"* Recoding ATM Transactions (X2) using K-rule")
                    analysis_lines.append(f"RECODE X2 (2 thru 5=1) (6 thru 9=2) (10 thru 13=3) (14 thru 17=4) (18 thru 21=5) (22 thru 25=6) INTO X2_Krule.")
                    analysis_lines.append(f"VALUE LABELS X2_Krule 1 '2-5' 2 '6-9' 3 '10-13' 4 '14-17' 5 '18-21' 6 '22-25'.")
                    analysis_lines.append(f"FREQUENCIES VARIABLES=X2_Krule.")
                    analysis_lines.append("")
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø£ÙŠ ØªØ­Ù„ÙŠÙ„ØŒ Ø¥Ù†Ø´Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø¹Ø§Ù…
        if not analysis_lines:
            if detected_vars:
                vars_str = ' '.join(detected_vars[:3])
                analysis_lines.append(f"* General analysis for variables: {vars_str}")
                analysis_lines.append(f"DESCRIPTIVES VARIABLES={vars_str}")
                analysis_lines.append("  /STATISTICS=MEAN STDDEV MIN MAX.")
            else:
                analysis_lines.append("* No specific analysis generated. Check variable detection.")
        
        return '\n'.join(analysis_lines)
    
    def generate_spss_header(self, df, data_analysis, filename):
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø£Ø³ ÙƒÙˆØ¯ SPSS Ø°ÙƒÙŠ"""
        header = f"""* =========================================================================
* SPSS SYNTAX FILE - INTELLIGENT GENERATION
* Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
* Data File: {filename}
* Rows: {data_analysis['summary']['total_rows']}
* Variables: {data_analysis['summary']['total_columns']}
* =========================================================================

* DATA DEFINITION AND SETUP
"""
        
        # ØªØ¹Ø±ÙŠÙ ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ©
        var_labels = []
        for var_name, suggested_label in data_analysis['suggested_labels'].items():
            var_labels.append(f"{var_name} '{suggested_label}'")
        
        header += "VARIABLE LABELS\n    " + " /".join(var_labels) + ".\n\n"
        
        # ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù‚ÙŠÙ… Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
        value_labels = []
        for var_name, var_info in data_analysis['variables'].items():
            if var_info['subtype'] in ['categorical_numeric', 'categorical_text']:
                if var_info['unique_values'] <= 10:
                    line = f"    /{var_name} "
                    
                    if var_name == 'X4':
                        line += "0 'No' 1 'Yes'"
                    elif var_name == 'X5':
                        line += "0 'No' 1 'Yes'"
                    elif var_name == 'X6':
                        line += "1 'City 1' 2 'City 2' 3 'City 3' 4 'City 4'"
                    elif var_info['subtype'] == 'categorical_numeric':
                        for val in var_info['values'][:10]:
                            line += f"{int(val)} 'Category {int(val)}' "
                    else:
                        for i, val in enumerate(var_info['values'][:5], 1):
                            truncated_val = str(val)[:20]
                            line += f"{i} '{truncated_val}' "
                    
                    value_labels.append(line)
        
        if value_labels:
            header += "VALUE LABELS\n"
            header += "\n".join(value_labels)
            header += ".\n\n"
        
        header += "EXECUTE.\n\n"
        header += "* =========================================================================\n"
        header += "* QUESTION ANALYSIS SECTION\n"
        header += "* =========================================================================\n\n"
        
        return header
    
    def create_download_link(self, content, filename, btn_text="ğŸ“¥ ØªÙ†Ø²ÙŠÙ„"):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø§Ø¨Ø· ØªØ­Ù…ÙŠÙ„"""
        b64 = base64.b64encode(content.encode()).decode()
        return f'<a href="data:file/txt;base64,{b64}" download="{filename}" style="text-decoration: none; padding: 10px 20px; background-color: #4CAF50; color: white; border-radius: 5px; font-weight: bold;">{btn_text} {filename}</a>'

# Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
def main():
    st.sidebar.title("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø°ÙƒÙŠ")
    
    # Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©
    st.sidebar.subheader("Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ÙƒØ´Ù Ø§Ù„Ø°ÙƒÙŠ")
    enable_smart_detection = st.sidebar.checkbox("ØªÙØ¹ÙŠÙ„ Ø§Ù„ÙƒØ´Ù Ø§Ù„Ø°ÙƒÙŠ Ø¹Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª", value=True)
    auto_suggest_labels = st.sidebar.checkbox("Ø§Ù‚ØªØ±Ø§Ø­ ØªØ³Ù…ÙŠØ§Øª Ø°ÙƒÙŠØ©", value=True)
    prevent_duplicates = st.sidebar.checkbox("Ù…Ù†Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©", value=True)
    
    st.sidebar.subheader("ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬")
    include_comments = st.sidebar.checkbox("Ø¥Ø¶Ø§ÙØ© ØªØ¹Ù„ÙŠÙ‚Ø§Øª ØªÙˆØ¶ÙŠØ­ÙŠØ©", value=True)
    show_variable_info = st.sidebar.checkbox("Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª", value=True)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆÙ„Ø¯
    generator = IntelligentSPSSGenerator()
    
    # Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    st.markdown("### ğŸ“ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
    col1, col2 = st.columns(2)
    
    with col1:
        data_file = st.file_uploader(
            "Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Excel/CSV)",
            type=['xlsx', 'xls', 'csv'],
            key="data_uploader"
        )
    
    with col2:
        questions_file = st.file_uploader(
            "Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø¦Ù„Ø© (TXT)",
            type=['txt'],
            key="questions_uploader"
        )
    
    if data_file and questions_file:
        try:
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if data_file.name.endswith('.csv'):
                df = pd.read_csv(data_file, encoding='utf-8')
            else:
                df = pd.read_excel(data_file)
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            questions_text = questions_file.getvalue().decode('utf-8')
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            with st.spinner("Ø¬Ø§Ø±Ù ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
                data_analysis = generator.analyze_dataset(df)
            
            st.success(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
            
            # Ø¹Ø±Ø¶ Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            with st.expander("ğŸ“Š Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", expanded=True):
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Ø§Ù„ØµÙÙˆÙ", data_analysis['summary']['total_rows'])
                with col2:
                    st.metric("Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª", data_analysis['summary']['total_columns'])
                with col3:
                    st.metric("Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙƒÙ…ÙŠØ©", len(data_analysis['summary']['numeric_vars']))
                with col4:
                    st.metric("Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©", len(data_analysis['summary']['categorical_vars']))
                
                st.write("**Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ù…Ø¹ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©:**")
                for var_name, suggested_label in list(data_analysis['suggested_labels'].items())[:10]:
                    var_info = data_analysis['variables'][var_name]
                    st.write(f"- **{var_name}**: {suggested_label} ({var_info['type']}, Ù‚ÙŠÙ… ÙØ±ÙŠØ¯Ø©: {var_info['unique_values']})")
                
                if len(data_analysis['suggested_labels']) > 10:
                    st.write(f"... Ùˆ {len(data_analysis['suggested_labels']) - 10} Ù…ØªØºÙŠØ±Ø§Øª Ø£Ø®Ø±Ù‰")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            questions = []
            lines = questions_text.split('\n')
            current_q = ""
            
            for line in lines:
                line = line.strip()
                if re.match(r'^\d+[\.\)]', line) or re.match(r'^\d+\.\s+', line):
                    if current_q:
                        questions.append(current_q.strip())
                    current_q = line
                elif current_q and line and not line.startswith('*'):
                    current_q += " " + line
            
            if current_q:
                questions.append(current_q.strip())
            
            # ØªØµÙÙŠØ© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙØ§Ø±ØºØ©
            questions = [q for q in questions if q and len(q) > 5]
            
            st.info(f"ğŸ“‹ ØªÙ… ØªØ­Ù„ÙŠÙ„ {len(questions)} Ø³Ø¤Ø§Ù„")
            
            # Ø¹Ø±Ø¶ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            with st.expander("ğŸ” ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„Ø£Ø³Ø¦Ù„Ø©", expanded=True):
                for i, q in enumerate(questions[:15], 1):
                    detected_vars = generator.detect_variables_in_question(
                        q, 
                        data_analysis['summary']['column_names'],
                        data_analysis
                    )
                    classifications = generator.classify_question(q)
                    
                    st.write(f"**Ø§Ù„Ø³Ø¤Ø§Ù„ {i}:**")
                    st.write(f"{q[:120]}{'...' if len(q) > 120 else ''}")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        if classifications:
                            st.caption(f"**Ø§Ù„Ø£Ù†ÙˆØ§Ø¹:** {', '.join(classifications)}")
                    with col2:
                        if detected_vars:
                            var_labels = [data_analysis['suggested_labels'].get(v, v) for v in detected_vars[:3]]
                            st.caption(f"**Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©:** {', '.join([f'{v} ({l})' for v, l in zip(detected_vars[:3], var_labels[:3])])}")
                    
                    st.write("---")
                
                if len(questions) > 15:
                    st.write(f"... Ùˆ {len(questions) - 15} Ø£Ø³Ø¦Ù„Ø© Ø£Ø®Ø±Ù‰")
            
            # Ø²Ø± ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯
            st.markdown("---")
            if st.button("ğŸš€ ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯ SPSS Ø§Ù„Ø°ÙƒÙŠ", type="primary", use_container_width=True):
                
                with st.spinner(f"Ø¬Ø§Ø±Ù ØªÙˆÙ„ÙŠØ¯ Ø£ÙƒÙˆØ§Ø¯ SPSS Ù„Ù€ {len(questions)} Ø³Ø¤Ø§Ù„..."):
                    
                    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø£Ø³
                    spss_code = generator.generate_spss_header(df, data_analysis, data_file.name)
                    
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø³Ø¤Ø§Ù„
                    processed_count = 0
                    skipped_count = 0
                    skipped_details = []
                    
                    progress_bar = st.progress(0)
                    
                    for i, question in enumerate(questions, 1):
                        progress_bar.progress(i / len(questions))
                        
                        question_code, skip_reason = generator.generate_spss_for_question(
                            i, question, df, data_analysis
                        )
                        
                        if question_code:
                            spss_code += question_code
                            processed_count += 1
                        else:
                            skipped_count += 1
                            skipped_details.append(f"Ø§Ù„Ø³Ø¤Ø§Ù„ {i}: {skip_reason}")
                    
                    # Ø¥Ø¶Ø§ÙØ© ØªØ°ÙŠÙŠÙ„
                    spss_code += f"""* =========================================================================
* END OF INTELLIGENT ANALYSIS
* Total Questions: {len(questions)}
* Successfully Processed: {processed_count}
* Skipped (to avoid duplicates): {skipped_count}
* Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
* =========================================================================
"""
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                    st.success(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {processed_count} Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­")
                    
                    if skipped_details:
                        st.warning(f"âš ï¸ ØªÙ… ØªØ®Ø·ÙŠ {skipped_count} Ø³Ø¤Ø§Ù„ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±")
                        with st.expander("ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ®Ø·Ø§Ø©"):
                            for detail in skipped_details:
                                st.write(detail)
                    
                    # Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ø§ØªØ¬
                    st.subheader("ğŸ“‹ ÙƒÙˆØ¯ SPSS Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
                    
                    # Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¶
                    show_full = st.checkbox("Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù…Ù„Ø§Ù‹", value=False)
                    
                    if show_full:
                        st.code(spss_code, language='text', height=600)
                    else:
                        code_lines = spss_code.split('\n')
                        preview_lines = code_lines[:200]
                        st.code('\n'.join(preview_lines), language='text')
                        
                        if len(code_lines) > 200:
                            st.info(f"Ø¹Ø±Ø¶ 200 Ø³Ø·Ø± Ù…Ù† Ø£ØµÙ„ {len(code_lines)}. Ù‚Ù… Ø¨ØªÙØ¹ÙŠÙ„ 'Ø¹Ø±Ø¶ Ø§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù…Ù„Ø§Ù‹' Ù„Ø±Ø¤ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù…Ù„Ø§Ù‹.")
                    
                    # Ù‚Ø³Ù… Ø§Ù„ØªÙ†Ø²ÙŠÙ„
                    st.markdown("---")
                    st.subheader("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø©")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.markdown(generator.create_download_link(
                            spss_code, "SPSS_Intelligent_Analysis.sps", "ğŸ“Š"
                        ), unsafe_allow_html=True)
                    
                    with col2:
                        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„
                        report = f"""ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ
========================
Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {data_file.name}
Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©: {len(questions)}
Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {processed_count}
Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ®Ø·Ø§Ø©: {skipped_count}

ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:
"""
                        for i, (fingerprint, info) in enumerate(generator.processed_questions.items(), 1):
                            report += f"""
{i}. Ø§Ù„Ø³Ø¤Ø§Ù„ {info['number']}:
   - Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {info['question']}
   - Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª: {', '.join(info['variables'])}
   - Ø§Ù„Ø£Ù†ÙˆØ§Ø¹: {', '.join(info['types'])}
"""
                        
                        st.markdown(generator.create_download_link(
                            report, "Analysis_Report.txt", "ğŸ“„"
                        ), unsafe_allow_html=True)
                    
                    with col3:
                        # Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
                        guide = f"""Ø¯Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙˆØ¯ SPSS Ø§Ù„Ø°ÙƒÙŠ
============================
1. Ø§ÙØªØ­ Ø¨Ø±Ù†Ø§Ù…Ø¬ SPSS
2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {data_file.name}
3. Ø§Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Window â†’ Syntax Editor
4. Ø§Ù„ØµÙ‚ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø±ÙÙ‚
5. Ø­Ø¯Ø¯ Ø§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù…Ù„Ø§Ù‹ (Ctrl+A)
6. Ø§Ø¶ØºØ· F5 Ø£Ùˆ Ø§Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„ØªØ´ØºÙŠÙ„

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„:
- ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {processed_count} Ø³Ø¤Ø§Ù„
- ØªÙ… ØªÙˆÙ„ÙŠØ¯ ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„
- ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ø°ÙƒÙŠØ© Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª
- ØªÙ… ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª

Ù†ØµØ§Ø¦Ø­:
- Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù†Ø§ÙØ°Ø© Output
- Ø§Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„
- ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø­Ø³Ø¨ Ø§Ø­ØªÙŠØ§Ø¬Ùƒ
"""
                        
                        st.markdown(generator.create_download_link(
                            guide, "User_Guide.txt", "ğŸ“"
                        ), unsafe_allow_html=True)
        
        except Exception as e:
            st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}")
    
    else:
        # ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ±Ø­ÙŠØ¨
        st.info("""
        ## ğŸ¯ Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ù„Ø£ÙƒÙˆØ§Ø¯ SPSS
        
        **Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:**
        âœ… **ÙƒØ´Ù Ø°ÙƒÙŠ Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª** - ÙŠØªØ¹Ø±Ù Ø¹Ù„Ù‰ "account balance" ÙƒÙ€ X1
        âœ… **ØªØ³Ù…ÙŠØ§Øª Ø°ÙƒÙŠØ©** - ÙŠØ¶Ø¹ ØªØ³Ù…ÙŠØ§Øª Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…ØªØºÙŠØ±Ø§Øª
        âœ… **ØªØ­Ù„ÙŠÙ„ Ù…Ø®ØµØµ** - ÙƒÙ„ Ø³Ø¤Ø§Ù„ ÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù…Ø®ØªÙ„Ù
        âœ… **Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±** - Ù†Ø¸Ø§Ù… Ø¨ØµÙ…Ø§Øª ÙŠÙ…Ù†Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        
        **ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
        1. **Ø­Ù…Ù‘Ù„ Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª** (Ù…Ø«Ù„ Data set 1.xlsx)
        2. **Ø­Ù…Ù‘Ù„ Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø¦Ù„Ø©** (Ù…Ù„Ù Ù†ØµÙŠ Ø¨Ø§Ù„Ø£Ø³Ø¦Ù„Ø©)
        3. **Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø°ÙƒÙŠ**
        4. **Ø­Ù…Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø©**
        
        **Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…:**
        - Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: Ù…Ù„Ù Excel Ø¨Ø£Ø¹Ù…Ø¯Ø© X1, X2, X3, X4, X5, X6
        - Ø§Ù„Ø£Ø³Ø¦Ù„Ø©: Ù…Ù„Ù Ù†ØµÙŠ Ø¨Ø£Ø³Ø¦Ù„Ø© Ù…Ø«Ù„:
          "Construct a frequency table for account balance"
          "Draw histogram for ATM transactions"
          "Calculate mean and standard deviation"
        """)

if __name__ == "__main__":
    main()
